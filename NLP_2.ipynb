{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMBz8HGHxdPgcDIuVgWIpz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYFfMBEJQQ99","executionInfo":{"status":"ok","timestamp":1739264854151,"user_tz":-330,"elapsed":596,"user":{"displayName":"Samruddhi Kathale","userId":"11453889790279965058"}},"outputId":"b69c5ad0-ba2e-4d8e-9b03-6fa276e8e4e4"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Bag-of-Words (Count Occurrence):\n","[[1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0]\n"," [0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0]\n"," [0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1]]\n","\n","Normalized BoW:\n","[[0.4472136  0.         0.         0.         0.         0.4472136\n","  0.4472136  0.4472136  0.         0.4472136  0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.40824829 0.40824829 0.40824829 0.         0.\n","  0.         0.         0.         0.40824829 0.40824829 0.\n","  0.40824829 0.         0.         0.        ]\n"," [0.         0.37796447 0.         0.         0.37796447 0.\n","  0.         0.         0.37796447 0.         0.         0.37796447\n","  0.         0.37796447 0.37796447 0.37796447]]\n","\n","TF-IDF:\n","[[0.46735098 0.         0.         0.         0.         0.46735098\n","  0.46735098 0.46735098 0.         0.35543247 0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.         0.3349067  0.44036207 0.44036207 0.         0.\n","  0.         0.         0.         0.3349067  0.44036207 0.\n","  0.44036207 0.         0.         0.        ]\n"," [0.         0.29651988 0.         0.         0.38988801 0.\n","  0.         0.         0.38988801 0.         0.         0.38988801\n","  0.         0.38988801 0.38988801 0.38988801]]\n","\n","Word2Vec Example:\n","[-1.7454965e-02  4.2603230e-03 -1.7470884e-03 -1.8638177e-02\n"," -1.8856285e-02 -2.8214359e-03  8.8648172e-03  7.4081421e-03\n"," -1.2997386e-02 -1.3746135e-02 -9.9988244e-03 -4.5736884e-03\n"," -1.4500575e-02 -1.9206636e-02 -5.4872585e-03 -1.6725682e-02\n"," -1.2077752e-02 -1.1341858e-02 -4.6882750e-03 -3.4139943e-03\n"," -1.7913997e-02 -1.4703989e-03  1.6305013e-02  1.5380859e-02\n"," -1.4412232e-02 -7.3336624e-03  6.2371041e-03 -1.9141445e-02\n","  2.9528784e-03  1.3048933e-02  1.1492839e-02 -1.7526124e-02\n"," -9.0342881e-03 -1.6280321e-02  9.1912749e-05  1.8527268e-02\n","  1.1946611e-02  1.0134616e-02  1.0122125e-02 -6.4858343e-03\n","  1.9104367e-02 -1.4712849e-02 -1.4540775e-02 -4.5307782e-03\n"," -1.5571213e-03 -6.4322068e-03 -1.1851717e-03  1.4977646e-02\n"," -1.3950372e-03 -3.2498813e-03]\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from nltk.tokenize import WhitespaceTokenizer\n","from gensim.models import Word2Vec\n","import numpy as np\n","nltk.download('punkt')\n","\n","# Sample data\n","corpus = [\"Natural Language Processing is amazing!\",\n","          \"Text data requires cleaning and processing.\",\n","          \"Tokenization and vectorization help with NLP tasks.\"]\n","\n","# Bag of Words\n","vectorizer = CountVectorizer()\n","bow_matrix = vectorizer.fit_transform(corpus)\n","print(\"Bag-of-Words (Count Occurrence):\")\n","print(bow_matrix.toarray())\n","\n","# Normalized Count Occurrence\n","bow_matrix_norm = bow_matrix.toarray() / np.linalg.norm(bow_matrix.toarray(), axis=1, keepdims=True)\n","print(\"\\nNormalized BoW:\")\n","print(bow_matrix_norm)\n","\n","# TF-IDF\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n","print(\"\\nTF-IDF:\")\n","print(tfidf_matrix.toarray())\n","\n","# Word2Vec Embedding\n","tokenized_corpus = [WhitespaceTokenizer().tokenize(sentence.lower()) for sentence in corpus]\n","word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=3, min_count=1, workers=4)\n","print(\"\\nWord2Vec Example:\")\n","print(word2vec_model.wv[\"nlp\"])  # Example word vector\n"]},{"cell_type":"code","source":[],"metadata":{"id":"a7ZmKohUQV8D"},"execution_count":null,"outputs":[]}]}