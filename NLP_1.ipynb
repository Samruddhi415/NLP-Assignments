{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhd5D5caspralbYlBkAq8+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WFdKSgS6-sd","executionInfo":{"status":"ok","timestamp":1739264552692,"user_tz":-330,"elapsed":9319,"user":{"displayName":"Samruddhi Kathale","userId":"11453889790279965058"}},"outputId":"e5a3262c-b31b-4780-f6ae-a62e5eb28880"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Whitespace Tokens: ['NLTK', 'is', 'an', 'amazing', 'library!', \"It's\", 'used', 'for', 'NLP', 'tasks.', 'Tokenization,', 'stemming', '&', 'lemmatization', 'are', 'essential.']\n","Punctuation Tokens: ['NLTK', 'is', 'an', 'amazing', 'library', '!', 'It', \"'\", 's', 'used', 'for', 'NLP', 'tasks', '.', 'Tokenization', ',', 'stemming', '&', 'lemmatization', 'are', 'essential', '.']\n","Treebank Tokens: ['NLTK', 'is', 'an', 'amazing', 'library', '!', 'It', \"'s\", 'used', 'for', 'NLP', 'tasks.', 'Tokenization', ',', 'stemming', '&', 'lemmatization', 'are', 'essential', '.']\n","Tweet Tokens: ['NLTK', 'is', 'an', 'amazing', 'library', '!', \"It's\", 'used', 'for', 'NLP', 'tasks', '.', 'Tokenization', ',', 'stemming', '&', 'lemmatization', 'are', 'essential', '.']\n","Porter Stemmed: ['nltk', 'is', 'an', 'amaz', 'library!', \"it'\", 'use', 'for', 'nlp', 'tasks.', 'tokenization,', 'stem', '&', 'lemmat', 'are', 'essential.']\n","Snowball Stemmed: ['nltk', 'is', 'an', 'amaz', 'library!', 'it', 'use', 'for', 'nlp', 'tasks.', 'tokenization,', 'stem', '&', 'lemmat', 'are', 'essential.']\n","Lemmatized: ['NLTK', 'is', 'an', 'amazing', 'library!', \"It's\", 'used', 'for', 'NLP', 'tasks.', 'Tokenization,', 'stemming', '&', 'lemmatization', 'are', 'essential.']\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer\n","from nltk.stem import PorterStemmer, SnowballStemmer\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# Sample text\n","text = \"NLTK is an amazing library! It's used for NLP tasks. Tokenization, stemming & lemmatization are essential.\"\n","\n","# Tokenization\n","whitespace_tokens = WhitespaceTokenizer().tokenize(text)\n","punctuation_tokens = WordPunctTokenizer().tokenize(text)\n","treebank_tokens = TreebankWordTokenizer().tokenize(text)\n","tweet_tokens = TweetTokenizer().tokenize(text)\n","\n","# Stemming\n","porter = PorterStemmer()\n","snowball = SnowballStemmer(\"english\")\n","porter_stemmed = [porter.stem(word) for word in whitespace_tokens]\n","snowball_stemmed = [snowball.stem(word) for word in whitespace_tokens]\n","\n","# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","lemmatized = [lemmatizer.lemmatize(word) for word in whitespace_tokens]\n","\n","# Output\n","print(f\"Whitespace Tokens: {whitespace_tokens}\")\n","print(f\"Punctuation Tokens: {punctuation_tokens}\")\n","print(f\"Treebank Tokens: {treebank_tokens}\")\n","print(f\"Tweet Tokens: {tweet_tokens}\")\n","print(f\"Porter Stemmed: {porter_stemmed}\")\n","print(f\"Snowball Stemmed: {snowball_stemmed}\")\n","print(f\"Lemmatized: {lemmatized}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"qfkCiKXGPrYJ"},"execution_count":null,"outputs":[]}]}