{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWURbsrfC5NY74OTKVVMwd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdAdKEGgRmwx","executionInfo":{"status":"ok","timestamp":1739265282074,"user_tz":-330,"elapsed":6862,"user":{"displayName":"Samruddhi Kathale","userId":"11453889790279965058"}},"outputId":"2de486dc-5d8e-4e50-c8ec-632dd8975e43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Transformer(\n","  (embedding): Embedding(10000, 512)\n","  (encoder_layer): TransformerEncoderLayer(\n","    (self_attn): MultiheadAttention(\n","      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","    )\n","    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","    (dropout1): Dropout(p=0.1, inplace=False)\n","    (dropout2): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-5): 6 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","        )\n","        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n","        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (fc_out): Linear(in_features=512, out_features=10000, bias=True)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class Transformer(nn.Module):\n","    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6):\n","        super(Transformer, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads)\n","        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)\n","        self.fc_out = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.encoder(x)\n","        return self.fc_out(x)\n","\n","# Hyperparameters\n","vocab_size = 10000\n","d_model = 512\n","num_heads = 8\n","num_layers = 6\n","\n","# Model Initialization\n","model = Transformer(vocab_size, d_model, num_heads, num_layers)\n","print(model)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"r1zUWBZ4SSSe"},"execution_count":null,"outputs":[]}]}